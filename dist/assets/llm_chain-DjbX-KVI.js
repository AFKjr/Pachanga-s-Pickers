import{b as h,a as m,d as b,e as f,R as o}from"./index-BkAnTRkv.js";class y extends h{constructor(){super(...arguments),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","output_parsers","default"]}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0})}static lc_name(){return"NoOpOutputParser"}parse(t){return Promise.resolve(t)}getFormatInstructions(){return""}}function d(r){return typeof r._llmType=="function"}function i(r){if(d(r))return r;if("bound"in r&&o.isRunnable(r.bound))return i(r.bound);if("runnable"in r&&"fallbacks"in r&&o.isRunnable(r.runnable))return i(r.runnable);if("default"in r&&o.isRunnable(r.default))return i(r.default);throw new Error("Unable to extract BaseLanguageModel from llmLike object.")}class n extends m{static lc_name(){return"LLMChain"}get inputKeys(){return this.prompt.inputVariables}get outputKeys(){return[this.outputKey]}constructor(t){if(super(t),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"prompt",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"llm",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"llmKwargs",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"outputKey",{enumerable:!0,configurable:!0,writable:!0,value:"text"}),Object.defineProperty(this,"outputParser",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.prompt=t.prompt,this.llm=t.llm,this.llmKwargs=t.llmKwargs,this.outputKey=t.outputKey??this.outputKey,this.outputParser=t.outputParser??new y,this.prompt.outputParser){if(t.outputParser)throw new Error("Cannot set both outputParser and prompt.outputParser");this.outputParser=this.prompt.outputParser}}getCallKeys(){return"callKeys"in this.llm?this.llm.callKeys:[]}_selectMemoryInputs(t){const e=super._selectMemoryInputs(t),u=this.getCallKeys();for(const s of u)s in t&&delete e[s];return e}async _getFinalOutput(t,e,u){let s;return this.outputParser?s=await this.outputParser.parseResultWithPrompt(t,e,u==null?void 0:u.getChild()):s=t[0].text,s}call(t,e){return super.call(t,e)}async _call(t,e){const u={...t},s={...this.llmKwargs},p=this.getCallKeys();for(const a of p)a in t&&s&&(s[a]=t[a],delete u[a]);const l=await this.prompt.formatPromptValue(u);if("generatePrompt"in this.llm){const{generations:a}=await this.llm.generatePrompt([l],s,e==null?void 0:e.getChild());return{[this.outputKey]:await this._getFinalOutput(a[0],l,e)}}const c=await(this.outputParser?this.llm.pipe(this.outputParser):this.llm).invoke(l,e==null?void 0:e.getChild());return{[this.outputKey]:c}}async predict(t,e){return(await this.call(t,e))[this.outputKey]}_chainType(){return"llm"}static async deserialize(t){const{llm:e,prompt:u}=t;if(!e)throw new Error("LLMChain must have llm");if(!u)throw new Error("LLMChain must have prompt");return new n({llm:await f.deserialize(e),prompt:await b.deserialize(u)})}serialize(){const t="serialize"in this.llm?this.llm.serialize():void 0;return{_type:`${this._chainType()}_chain`,llm:t,prompt:this.prompt.serialize()}}_getNumTokens(t){return i(this.llm).getNumTokens(t)}}export{n as LLMChain};
